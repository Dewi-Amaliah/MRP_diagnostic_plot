---
chapter: 3
knit: "bookdown::render_book"
---

```{r setup-chap3, include=FALSE, cache = FALSE}
library(knitr)
read_chunk(here::here("case_study/analysis/analysis_code/mrp_fitting.R"))
read_chunk(here::here("case_study/analysis/analysis_code/mrp_vis.R"))
```


```{r pkgs-chap3}
```

# Case Study: Application of MRP in Presidential Voting Estimation {#ch:case-stud}

The majority of MRP applications are used in the context of estimating public opinion in the social and political sciences, although, in recent developments  MRP has also been used in other fields, for example, health and environmental studies. When first introduced by @Gelman97poststratificationinto, MRP was applied to generate state estimation of the 1988 U.S. presidential election. Various subsequent studies also made presidential voting the case of interest. We recorded at least seven articles (@GelmanAndrew2014HBAC; @GhitzaYair2013DIwM; @KiewietdeJongeChadP2018PSPE; @LauderdaleBenjaminE2020Mppf; @LeiRayleigh2017T2EA; @ParkDavidK2004BMEw; @WangWei2015Fewn) included in the systematic literature review in Chapter 2 that also applied MRP to presidential election estimation. In this chapter, we will also apply MRP to estimate the 2016 U.S. presidential voting outcome, specifically the probability of voting for Donald Trump in this election. This also allows us to compare MRP estimates with the actual value of the Trump votes that are already available. In this case study, we use the Cooperative Congressional Election Study (CCES) 2016 data [@cces_data] as the survey data and the American Community Survey data 2015-2017 [@acs_data] as the population/ poststratification data.

## Data

```{r acs-cces-read}
```

```{r ct-data}
```


### Cooperative Congressional Election Study (CCES) 2016 

CCES is an annual survey that aims to capture Americans' view on Congress, their voting behavior and experience with regards to political geography, social, and demographic context [@cces_data]. In 2016, the CCES covers 64,600 samples spread over 51 states. Accordingly, @cces_data suggest that the data is precise enough to measure the distribution of voters' preference in most states. In addition, beyond it's large sample size, CCES is regarded to be a desirable dataset because it measures vote preference before and after the election so that it is more reliable compared to a single question format [@kuriwaki]. 

To fit MRP models, we use several variables from this survey. To obtain the data from the CCES website, we utilize an R package, `ccesMRPprep` [@ccesmrpprep]. One of the advantages of using this package is that the data has been pre-processed in particular for MRP purposes, in this case, we use the `ccc_std_demographics` function. Also, the variable names are already recoded so it has more interpretable names. The code to get the data is available in the supplementary materials of this thesis. 

Throughout this demonstration, we estimate the proportion of voters who turned out to vote for Trump in the 2016 U.S. Presidential Election. We choose this outcome following other demonstrations (e.g. @kuriwaki; @MengXiao-Li2018Spap) precisely because the population quantity is observed after the survey is run. That allows us to validate my estimates against a ground truth. To visualise the implication of  different model specification, we also choose other two outcomes, which are vote preference and party identity. In CCES 2016 these variables named candidate voted for (`CC16_410a`), the vote preference/intention (`CC16_364c`),and party identity (`pid3` including leaners who are coded as Independents in pid3 but expressed leaning towards a party in `pid7`). Table \@ref(tab:outcome-table) shows the distribution of answers in those three variables. In `ccesMRPprep`, these variables have been renamed to `intent_pres_16`, `voted_pres_16`, and `pid3_leaner`, respectively. It is worth noting that the MRP models we would like to build use binary responses. As we are comparing to the US presidential election, we would like a variable that represents whether the respondents vote for Trump/Republican or not.

```{r outcome-table}
```


Further, the geography and demographic variables used as covariates in the models are `state`, `age`, `gender`, `education`, and `race`. Table \@ref(tab:covariate-tables) shows the distribution of categories/levels of `age`, `gender`, `education`, and `race`. Initially, age recorded as integers but we transformed it into five age groups. Also, `education` and `race` have more levels in the original data but are collapsed to have to obtain fewer levels. In particular, we use the standard/default categorisation in the `ccesMRPprep` package. The proportion of people answered the survey based on the state is displayed in the appendix of this thesis (\@ref(apd-state)). 

```{r covariate-tables}
```


### American Community Survey (ACS) 2015-2017

In this study, we use the ACS 2015-2017 data as the poststratification data. The ACS is a large, survey of the American population conducted by the census bureau and covering jobs and occupations, demographic and citizenship, educational attainment, homeownership, and other topics [@acs_data_about]. The ACS uses monthly probabilistic samples to produce the annual estimates. The ACS is desirable data to represent the U.S. population since the coverage rate, a measure on how well does the survey cover population, for the 2015-2017 ACS is 92.4%, 91.9%, 91.6%, respectively [@acs_coverage_rate]. However, it is also worth noting that the population of interest of the ACS (American population) and the population we are interested in (voting population) is different (the ACS measures the general US population, while the CCES wants to study the behavior of the U.S. adult citizens who turned out to vote), and therefore, bias might always be presented.

To construct the desired poststratification matrix, we need the individual data of the ACS instead of the aggregated statistics. To do this, we use the 1-year Public Use Microdata Sample (PUMS), which carries the information/records of individual people on a yearly basis, appropriately deidentified. The 1-year PUMS data reflects approximately one percent of the U.S. population [@pums_metadata]. Therefore, in this study, we use three years periods of the ACS 1-Year PUMS from 2015-2017 instead of 2016 only to get a better and more stable representation of the American population. Every individual in the data has a weight (`PWGTP`). Since we use three years period, this weight is then divided by 3 to obtain a population total that matches the full population total.  

The data is publicly available on the [U.S. Census Bureau website](https://www.census.gov/programs-surveys/acs/microdata/access.2015.html). We downloaded the data in a .csv format (csv_pus.zip) year by year (2015-2019) through access on [FTP site](https://www2.census.gov/programs-surveys/acs/data/pums/2015/1-Year/). After that, we did a data pre-processing to bind the three years of the PUMS data. We only use some variables in this data for the MRP-purposes, i.e., unique identifier of the person (`SERIALNO`), state (`ST`), weight (`PWGTP`), education (`SCHL`), sex (`SEX`), race (`RAC1P`), Hispanic origin (`HISP`), and age (`AGEP`). We also did a data munging to recode and collapse some categories in these variables. Note that the `RAC1P` did not record for Hispanic ethnicity. Hence, we introduce a new category here, Hispanic, identified if the person answers other than "1" in the `HISP` variable. Table \@ref(tab:acs-response-freq) shows the categorised response of the variables obtained from the ACS, i.e., age, race and ethnicity, and education (see Appendix \@ref(apd-state) for state). Also, notice that we get some NA values in education. This is actually the education level of under-school-age respondents. We omit respondents less than 18 years old in the MRP models as the (CCES) targets an adult population. Accordingly, the NAs in education response will be eventually omitted as well. The detailed code of the data pre-processing is available in the supplementary materials of this thesis. 

```{r acs-response-freq}
```


## Model Specifications {#spec}

```{r survey-pop-data}
```

In Chapter 2, we found that the diagnostic plots shown in many articles compare MRP estimates with other estimates. One version of this compares several MRP estimates with different model specifications. To allow us to make the same comparisons in this case study, we build five different MRP models as follows.


**Baseline model** 

We begin the model fitting with the baseline model. In this model, we set the binary outcome as whether the respondents vote for Trump or not in the 2016 election. Therefore, we transform the response of `voted_pres_16` into a binary variable called `vote` ,i.e, if the value of `voted_pres_16` is "Donald Trump", then `vote` variable coded to "yes", otherwise "no". The NA values in `voted_pres_16` will stay as NA in the new `vote` variable. Tthe distribution of the baseline model's outcome variable is displayed in Table \@ref(tab:vote-dist). 

```{r vote-dist}
```

The demographic predictors used are `age`, `gender`, `state`, and `race`. As seen in Table \@ref(tab:covariate-tables), `race` has 6 categories, i.e., `White`, `Black`, `Hispanic`, `Asian`, `Native American`, and `All Other`. In the baseline model, we collapsed the `Native American` and `All Other` into `Other`. Meanwhile, the levels of `age`, `gender`, `state` stay the same in the levels displayed in Table \@ref(tab:covariate-tables). The baseline model equation is: 

\begin{equation} 
\begin{split}
\Pr(vote_{j[i]} = 1) & = logit^{-1}\left(\beta_0 + \alpha^{age}_{m[i]} + \alpha^{gender}_{m[i]} + \alpha^{state}_{m[i]} + \alpha^{collapsed\ race}_{m[i]}\right), \\
for\ i  & = 1, ...., n, \\
\beta_0  & \sim t(3,0,2.5) \\
\alpha^k_m  & \sim N(0,\sigma_k)
\end{split}
(\#eq:baseline-model)
\end{equation}


and $vote_{j[i]}$ is the binary outcome (1 = yes, 0 = no) for individual $i$ in poststratification cell $j$. $\beta_0$ is the intercept. $\alpha^{age}_{m[i]}$, $\alpha^{gender}_{m[i]}$, $\alpha^{state}_{m[i]}$, and $\alpha^{collapsed\ race}_{m[i]}$ are the random effects for `age`, `gender`, `state`, and `collapsed race`, respectively. The subscript in each coefficient represents the category of the $i-th$ respondent, such as, $\alpha^{collapsed\ race}_{m[i]}$ takes value from {$\alpha^{collapsed\ race}_{White}$, $\alpha^{collapsed\ race}_{Black}$, $\alpha^{collapsed\ race}_{Hispanic}$, $\alpha^{collapsed\ race}_{Asian}$, and $\alpha^{collapsed\ race}_{Other}$}. Each random effect has an independent prior distribution, such as, $\alpha^{collapsed\ race}_{m}$ ~ $N(0, \sigma^2_{collapsed\ race})$ and $\beta_0$ ~ $t(3, 0, 2.5)$. Here, we use the default prior because we only want to compare models for visualisation purpose instead of looking for the best model for estimation.  

\vspace{\baselineskip}

**Model with `education` as additional covariate**

Next, we create a bigger model by adding `education` as additional covariate to the baseline model. The levels of `education` is also displayed in Table \@ref(tab:covariate-tables). Hence, the model specification is:


\begin{equation} 
\begin{split}
\Pr(vote_{j[i]} = 1) &= logit^{-1}\left(\beta_0 + \alpha^{age}_{m[i]} + \alpha^{gender}_{m[i]} + \alpha^{state}_{m[i]} + \alpha^{collapsed\ race}_{m[i]} + \alpha^{education}_{m[i]}\right), \\
for\ i &= 1, ...., n.
\end{split}
(\#eq:model2)
\end{equation}


\vspace{\baselineskip}

**Model with original race categories**

This model is essentially the same with baseline model, except that there are more race categories, which are `White`, `Black`, `Hispanic`, `Asian`, `Native American`, and `All Other`. The model equation is: 

\begin{equation} 
\begin{split}
\Pr(vote_{j[i]} = 1) = logit^{-1}\left(\beta_0 + \alpha^{age}_{m[i]} + \alpha^{gender}_{m[i]} + \alpha^{state}_{m[i]} + \alpha^{original\ race}_{m[i]}\right), for\ i = 1, ...., n.
\end{split}
(\#eq:model3)
\end{equation}


**Model with different outcomes**

**Vote intention/preference**

This model mimicks the model in Equation \@ref(eq:model2), except that we have a different outcome or response variable. The response here is whether the respondent intends to vote for Trump (`yes`) or not (`no`) (rather than whether they reported they voted for Trump). It is transformed from `intent_pres_16` variable in the CCES data to a new variable called `intent`. If the value of `intent_pres_16` is "Donald Trump (Republican)", then `intent` variable coded to "yes", otherwise "no". The NA values in `intent_pres_16` will stay as NA in the new `intent` variable. The distribution of observed "no", "yes", and NA in this variable is shown in Table \@ref(tab:intent-dist). 

```{r intent-dist}
```


The model is specified as follows:

\begin{equation} 
\begin{split}
\Pr(intent_{j[i]} = 1) &= logit^{-1}\left(\beta_0 + \alpha^{age}_{m[i]} + \alpha^{gender}_{m[i]} + \alpha^{state}_{m[i]} + \alpha^{collapsed\ race}_{m[i]} + \alpha^{education}_{m[i]}\right), \\
for\ i &= 1, ...., n.
\end{split}
(\#eq:model4a)
\end{equation}

**Party identity**

Beside vote intention, another outcome is the party identity in terms of whether the respondents identify themselves as Republican or not. This variable is derived from `pid3_leaner` variable and referred as `party`. If the value of `pid3_leaner` is "Republican (Including Leaners)", then `party` variable coded to "Republican", otherwise "not Republican". The NA values in `pid3_leaner` will stay as NA in the new `party` variable. The distribution of this outcome variable is displayed in Table \@ref(tab:party-dist).

```{r party-dist}
```

The specification of covariates is also the same with model in Equation \@ref(eq:model2). 

\begin{equation} 
\begin{split}
\Pr(party_{j[i]} = 1) &= logit^{-1}\left(\beta_0 + \alpha^{age}_{m[i]} + \alpha^{gender}_{m[i]} + \alpha^{state}_{m[i]} + \alpha^{collapsed\ race}_{m[i]} + \alpha^{education}_{m[i]}\right), \\
for\ i &= 1, ...., n.
\end{split}
(\#eq:model4b)
\end{equation}


The estimates from the multilevel model is then used for the second stage of MRP, which is poststratification. As the explanation in Section \@ref(overview), poststratification is essentially taking the weighted average of the cell-wise posterior estimates with the  size of each cell in the population table as the weight [@GaoYuxiang2021IMRa]. For example, the poststratification estimates of people who completed High School or less and voted for Trump in the 2016 presidential election is:

\begin{equation} 
\begin{split}
\theta_S = \frac{\sum_{j\in S}N_j\theta_j}{\sum_{j\in S}N_j},
\end{split}
(\#eq:poststrat-observed)
\end{equation}

where $\theta_S$ corresponds to the proportion of 45 to 64 years old of Black Men attained High School or less (HS or Less) in Alabama who respond to "yes" in the `vote` variable and $N_j$ and $\theta_j$ are the size of cell corresponds to this sub-population category in the poststratification table and the posterior estimates of this sub-population category, respectively. 


## Model Preparation and Fitting {#prep}


```{r questionmap}
```

```{r tabulation}
```

The MRP models require synchronous measurements between survey and population data. To achieve this, we need to map the survey data to the population data. In this study, the model preparation and survey-population data mapping is conducted with an R package, `mrpkit` [@mrpkit]. This package allows the transparent and reproducible workflow to build MRP model, from the data mapping until the prediction stage, including the model specification setting. This package is not the product of this study but I am one of its authors. The detailed code to build the MRP models is available in the supplementary materials of this thesis. 

After mapping the survey and population data, we can obtain a poststratification table, the first five rows of which is displayed in Table \@ref(tab:post-strat-table)

```{r post-strat-table}
```


Next, we implement a Bayesian multilevel model using `brms` [@brms] to fit the model and obtain the posterior distributions of the parameters. `brms` itself incorporates either rstan  or cmdstanr [@cmdstanr] as the backend, which in turn wrap the probabilistic programming language Stan [@stan]. We use 4000 samples of posterior distribution generated with 4 independent chains. Since this task is computationally heavy and time-consuming, we conduct it using [Monashâ€™s High Performance Cluster (HPC)](https://docs.monarch.erc.monash.edu/MonARCH/aboutMonArch.html). 

## Results and Discussion

```{r read-map-fits}
```


The MRP estimates from these models will be visualised in this subsection. We will illustrate the implications of current visualisation practices and discuss the possibility for improvement using these estimates. We will divide the discussion with regards to communication and diagnostic plot as we did in in the systematic literature review (Subsection \@ref(com-prac)).  

### Visualisations for communication purposes

One of the most widely used graphs to communicate MRP estimates is a choropleth (see Figure \@ref(fig:common-plots)). Choropleth is colored, shaded, or graded to display a spatial pattern of a certain variable. For example, blue and red are used to represent states with more Democrat and Republican voters, respectively, as seen in @GhitzaYair2013DIwM. A color gradient is also used to convey a more detailed message, for example, the state-wise MRP estimates of pro-environment opinion as seen in @EunKimSung2018Epoi. The greener the shade, the more proportion of people support pro-environmental policy. These two examples also show the use of color with respect to the meaning that people generally perceive, i.e., green is often associated with the environment, and blue is often associated with Democrats.  

In this case study, we create a choropleth of MRP estimates of the probability of voting for Trump (\@ref(fig:choro)) in the U.S. 2016 election using the baseline model. We create the same choropleth that is commonly shown based on our findings in the literature review.

```{r choro, fig.cap = "MRP estimates of probability of state vote for Trump in the U.S. 2016 presidential election using the baseline model. The deeper the blue shade the lower Trump's vote share in the corresponding state, while the deeper the red, the higher Trump's vote share. It is shown that the baseline model predicts that Trump has less than 50 percent vote share in almost every state in the U.S."}
```

The choropleth as seen in Figure \@ref(fig:choro) conveys that the baseline model predicts that Trump has less than 50 percent of vote share in almost every state in the U.S. Regardless of whether this model has a good fit or not, the message that this graph tries to convey using color is quite easy to perceive. We can see a blue-shaded U.S. map, meaning that the Democrat candidate wins the majority of votes in most states. However, this takeaway is general, while the purpose of MRP, is to give more detailed information about sub-populations. From the map, we can see that there is only one state that has a red tint. However, the readers, especially those unfamiliar with the U.S. map, will probably not know which state this is unless the states are labeled with their name. 

Another critic on the choropleth is also stated by @statgraph. He argues that choropleth is problematic as polygons with small areas are difficult to observe. In fact, these areas sometimes carry particular information. For example, small geopolitical areas can represent a high density of population. He argues that one alternative to overcome this problem is to replace a choropleth with a cartogram in which the area is distorted so that its proportional to the value of the variable it represents. Unfortunately, there is no single visualisation among the articles reviewed that utilize this kind of visualisation.

Choropleth maps also only display point estimates, which is only one component of our analysis. Uncertainty should also be considered when visualising data, particularly estimation results, as there is always variability in these [@tukey; @MIDWAY2020100141; @HullmanJessica2019IPoE]. In this case, a dot plot with a confidence or credible interval could be used to visualise MRP estimates, for example, as seen in @EnnsPeterK2013POit. We can see that there is a reasonably high percentage of the usage of dot plot with uncertainty in the articles we reviewed. From the 34 dot plots found, 26 (about 76%) of them display uncertainty. However, compared to the overall number of communication plots, the portion of the dot plot with uncertainty is only about 20%.

### Visualisation for diagnostic purposes {#vis-purp}

**Displaying Comparison of Estimation Methods**

According to @tukey, one of the graphic's purposes is for comparison. In MRP visualisation practice, the estimates from various estimation methods are often compared. Here, we compare state-level MRP estimates with raw and weighted estimates compared to their closeness to the ground truth (actual Trump vote shares), which we obtained from R-package `ddi` [@ddi]. The common aesthetic used to display this kind of purpose in the reviewed articles is a scatter plot (around 31% of the total diagnostic plots).  

There is an unwritten "rule of thumb" that when displaying two variables in a scatter plot, the horizontal axis displays the predictor, while the outcome is put in the vertical axis [@gelmanunwin]. Regarding MRP visualisation, this "convention" could be translated by putting the estimates in the y-axis and the actual value in the x-axis, although the practice is sometimes interchangeable (see Figure \@ref(fig:common-axis)). We also observe that some of the reviewed scatter plots show performance metrics, such as RMSE and MAE in  @MengXiao-Li2018Spap. Hence, in Figure \@ref(fig:scatter-est-method), we also display these. Most scatter plots we reviewed did not display uncertainty (see Figure \@ref(fig:common-plots)). Here, we add uncertainty to each point estimates. In addition, we use color-blind-friendly color schemes to distinguish the estimation methods as mentioned by @vanderplas and @statgraph. 

```{r est-comparison}
```

```{r scatter-est-method, fig.cap = "Comparison between various estimates (raw, weighted, and MRP) with the actual vote share for Trump as observed in the election. The MRP model used here is the model with education as additional predictor. The points represent states with the 95 percent credible or confidence interval (depending on the method), while color represents the estimation method used. Weighted estimates is accurately predict the actual value of Trump's vote share.", fig.height = 5, fig.width = 5}
```

From this visualisation, we can clearly see that the weighted estimates, as seen in Figure \@ref(fig:scatter-est-method), are observed to be the most accurate. It is actually an expected result, as according to @cces_data, the CCES's weights are poststratified to match the statewide election results. 

However, a scatter plot is appropriate when the purpose is to allow the readers to discern the general information about the relationship *shape* between two variables rather than inference about *individual data points* [@saundra]. Hence, if the purpose is to inspect which states are least accurately estimated, the scatter plot would not be suitable. One option to help is to add labels to the points but these labels would be overlapped and hard to read in this case. Again, the dot plot could be used as an alternative to convey state-wise information, as seen in Figure \@ref(fig:dot-est-method). Here, instead of conveying the estimates, we use their deviance from the actual value of Trump's vote share, i.e., the $Estimates - Actual\ value $. We also display the states in descending order of the actual value of Trump votes, i.e., from the most "red" states to the most "blue" states. 

```{r dot-est-method, fig.height=7, fig.width=6, fig.cap = "The deviance of estimated values from the actual value of Trump's vote share. The states in the vertical axis are ordered from states with the highest to the lowest Trump's vote share with regards to the ground truth value. The color represents the estimation methods. Again we saee that weighted estimates show the smallest deviance from the ground truth. This figure shows a pattern in which the more conservative the state, the bigger the deviance."}
```

Using this graph, we can get the same information regarding estimate accuracy. However, we can also display other information related to the estimation error, which can then be compared across estimation methods. It shows that the more conservative the state, the higher the error. This pattern could indicate that the survey data adjustments are not sufficient to correct for sampling bias, or potentially bias between the population we poststratify to and the voting population.


**Displaying Comparison of Model Specifications**

Aside from comparing estimation methods(weighed, raw or direct estimates and mrp), we could also compare between different model specifications. Revisiting on what @WickhamHadley2015VsmR stated, model visualisation could answer how the model fits change as the data changes. The following graphics will demonstrate this purpose.  

Similar to previous plots, the comparison shown in Figure \@ref(fig:state-wise-scatter) is displayed using a scatter plot. Here we compare between the MRP estimates and the actual Trump's vote share. Since there are five model specifications, we use the small-multiple principle [@MIDWAY2020100141], i.e., displaying the model fits with facets.

```{r met-fun}
```

  
```{r state-wise-scatter, fig.width=8, fig.height=7, fig.align='left', fig.cap="Comparison between MRP estimates and the actual Trump's vote share faceted by model specification. The point represents the state. Panel A represents the fit of baseline model; B represents the model with education as additional predictor; C represents the model with more race categories; A, B, and C have the same response variable, vote, while D and E represente the model with different outcome, which are vote intention and party identity, respectively. The covariates used in model D and E are the same with the covariates of model B. We can see that all the models underestimate the actual Trump's vote share."}
```

Using this graph, we can observe that the fit changes as the specification changes. The $45^{\circ}$ line assists the readers in inspecting whether the fit is underestimated or overestimated. Even though almost all of the fits are underestimated, we can see that the bigger model, i.e., the model with education as an additional covariate, has a better fit than the other models (also shown by its MAE). Models in panels D and E, which are models with different outcomes, are less accurate, which is understandable as the benchmark is the actual Trump vote-share which is more aligned with the other outcome,`vote`. 

In addition to estimation by small geographical area, MRP is also often used to estimate population by demographic subsets. We use violin plots to compare how the subpopulation estimates change as the model specification changes in the following visualisation. We use the violin plot as it can show the distribution of the estimates, although this plot was never observed in the articles we reviewed. It allows the reader to observe the variability and uncertainty of the rather then just the point estimates of summary statistics. 

```{r violin-data}
```

```{r violin-levels-plot, fig.width=4.5, fig.height=8.5, fig.cap = "The comparison of MRP estimates by model specification. This panel shows the demographic variables estimated. Panel A, B, and C represents Trump's vote share by race categories, education level, and age group, respectively."}
```

Figure \@ref(fig:violin-levels-plot) shows the distribution of the response variable, which is probability of vote for Trump for each demographic levels regardless the geographic levels or the states where the voters live. This figure illustrates how the estimates will be different as the result of different covariates used.   For example, in Panel A, the range of probability of vote for Trump of Native Americans in the model with more race categories is wider than the model that collapsed Native American and All Other as one race category. We can also see that the median of the outcome in All Other race categories is slightly different in the two models. A more pronounced difference could also be observed in Panel B which compare the baseline model with the model with education level as additional covariate. Incorporating education into the model results in a different pattern compared to the baseline model, i.e., the higher the education level, the less probability of voting for Trump. In Panel C, we can see the same trend for the three model fits, where the older age-groups tend to be more likely to vote for Trump. However, the median of model with additional adjustment variables is slightly higher in all age groups. 

**Visualising Metrics**

Metrics are the performance measure of the model in estimating the ground truth. In this case, however, since the benchmark is not the actual value due to the absence of Trump's vote share in demographic levels, the term performance is not quite correct, but rather difference between these two models. We will still display graphs for these metrics, though, as an illustration of performance visualisation.   

MAE and bias are predominantly used in most of the articles as the model performance criteria. Essentially, they give the same interpretation, which is how precise the model is in estimating the actual value. We also observe that correlation is frequently used in practice. Some studies also incorporate MSE/RMSE to measure their model performance. 

@WarshawChristopher2012HSWM display correlation and MAE between state wide estimates and ground truth in a single graph by faceting it. Hence, we make a like-wise plot with a slight modification in the correlation (Figure \@ref(fig:cor-race-plot)). The current practices display correlation as it is. When a graph only displays a single metric, there will be no distortion of its interpretation. However, the graph would be quite hard to read if we facet MAE or MSE/RMSE and correlation because the scales are not interpreted in the same way. For MAE and MSE/RMSE, the lower the value, the better the accuracy. In contrast, a higher correlation coefficient is more desirable. To make these scales more interpretable in the following graph, we display $1 - correlation$ instead so that the interpretation is unidirectional. We also set the free "scale" so that the consistency of performance of estimates by subpopulation could be examined. Setting the display this way applies the cognitive principle of best graphical practice as stated in @vanderplas, in which data is better presented in a way that allows the reader to compare more accurately. 

```{r perf-metrics}
```

```{r metrics-plot-fun}
```


```{r cor-edu-data}
```

```{r ci-len-data}
```


```{r cor-race-plot, fig.width=5, fig.height=6, fig.cap = "Metrics of model with more race categories. This figure in only an illustration as it uses the benchmark is the baseline model, not the ground truth. Each panel shows different metrics (Correlation, Root Mean Square Deviance, and Mean Absolute Difference). The statewide categories means the state-wise metrics regardless of the race categories. Native American and All Other are the population subset with the biggest difference to the baseline model."}
```

From Figure \@ref(fig:cor-race-plot) suggests that Native American and All Other race categories are consistently estimated to have a higher MAD and lower correlation with the baseline model. It is sensible because the baseline model collapses these categories as one covariate. This figure also illustrates how model visualisation answers whether the model is uniformly good or it is only fit for specific regions, in this case, race categories [@WickhamHadley2015VsmR].

In addition to the metrics displayed in Figure \@ref(fig:cor-race-plot), we also propose alternative metrics that do not exist in the reviewed articles, namely the length of the error bar, in this case, is the 95% credible interval. It is obtained by subtracting the 2.5% quantile from the 97.5% quantile of the estimates. The idea is that there is a bias-variance trade-off in MRP, and metrics, such as MAE, only take bias into account. Therefore, in Figure \@ref(fig:race-ci-len), we display the difference and ratio between the credible interval length of the model with more race categories and the baseline model. This measure will compare the variability of two model fits. If the value of credible interval length difference is near zero, then the variability of two model fits is pretty much the same. A ratio near to 1 could be interpreted in the same way. 

```{r race-ci-len, out.width="90%", fig.cap = "Credible interval length comparison between the model with more race categories and the baseline model. The left panel displays the mean of length difference, while the right panel display the mean of credible interval ratio. The credible interval length of Native Americans on model with more race categories is 1.5 wider than the baseline model."}
```

Figure \@ref(fig:race-ci-len) shows that the estimated interval of Trump's vote share in Native American categories is 1.5 wider compared to the baseline model, while other race categories generally have the same length of the credible interval with the baseline model. Hence, using this type of graph, we can sumarise that Native Americans' Trump's vote share estimate might be more uncertain when compared to other other race categories.  

To sum, this demonstration shows that graphical display can help us to understand the model better. For example, the graphs have shown us that the difference in covariates or model specification could result in reasonably different estimates. 